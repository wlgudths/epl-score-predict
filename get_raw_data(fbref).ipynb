{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape_fbref()\n",
    "\n",
    "### Fbref.com에서 프리미어 리그 경기(매치) 데이터를 스크래핑하는 함수\n",
    "\n",
    "- param start_year = 시작 연도 (기본값 :2004)\n",
    "- param end_year = 종료 연도 (기본값 : 2024)\n",
    "- param output_file : CSV 파일 이름 (기본값 : 'epl_match_data_fbref.csv')\n",
    "- data_generator() : URL 리스트를 순회하며 각 URL에서 데이터를 스크래핑하는 제너레이터 함수\n",
    "- return : 스크래핑된 데이터를 포함하는 DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RCp2zwVZSk6y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape_fbref 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_fbref(start_year=2004, end_year=2024, output_file='epl_match_data_fbref.csv'):\n",
    "    # fbref.com에서 프리미어 리그 매치 데이터를 가져오기 위한 기본 URL 형식\n",
    "    base_url = 'https://fbref.com/en/comps/9/{}/schedule/{}-Premier-League-Scores-and-Fixtures'\n",
    "    \n",
    "    # 2004년부터 2023까지의 시즌에 대한 URL 리스트 생성\n",
    "    urls = [base_url.format(f'{season}-{season+1}', f'{season}-{season+1}') \n",
    "                 for season in range(start_year, end_year)]\n",
    "    \n",
    "    def data_generator():\n",
    "        # tqdm을 사용하여 진행 상황을 시각화\n",
    "        for url in tqdm(urls, desc='Scraping data'): \n",
    "            # 과도한 요청을 보내지 않기 위해 3 ~ 6초 사이의 랜덤한 시간 동안 대기\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "            # yield from 구문을 사용하여 pd.read_html(url)의 결과를 한 번에 하나씩 반환\n",
    "            yield from pd.read_html(url) \n",
    "\n",
    "    combined_df = pd.concat(data_generator(), ignore_index=True)\n",
    "\n",
    "    # 현재 파일의 디렉토리 경로\n",
    "    current_dir = os.getcwd()\n",
    "    # 'raw_data' 폴더 경로\n",
    "    raw_data_dir = os.path.join(current_dir, 'raw_data')\n",
    "\n",
    "    # 'raw_data' 폴더가 없으면 생성\n",
    "    os.makedirs(raw_data_dir, exist_ok=True)\n",
    "\n",
    "    # 'raw_data' 폴더 안에 CSV 파일 저장\n",
    "    output_path = os.path.join(raw_data_dir, output_file)\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f'{output_file} 데이터가 {output_path}에 저장되었습니다.')\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    result_df = scrape_fbref()\n",
    "    print(f'데이터 스크래핑 완료. 총 {len(result_df)} 행의 데이터가 저장되었습니다.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
